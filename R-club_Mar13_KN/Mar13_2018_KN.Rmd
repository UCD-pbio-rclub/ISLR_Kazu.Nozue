---
title: "Feb_27_2018"
author: "Kazu"
date: "3/13/2018"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(error = TRUE)
library(ISLR);library(leaps)
```
# Chapter 6 Lab 3: PCR and PLS Regression
# Principal Components Regression
```{r}
#Again, ensure that the missing values have been removed from the data, as described in Section 6.5.
dim(Hitters)
sum(is.na(Hitters$Salary))
Hitters=na.omit(Hitters)
dim(Hitters)
sum(is.na(Hitters))
x=model.matrix(Salary~.,Hitters)[,-1]
y=Hitters$Salary
set.seed(1)
train=sample(1:nrow(x), nrow(x)/2)
test=(-train)
y.test=y[test]

library(pls)
set.seed(2)
pcr.fit=pcr(Salary~., data=Hitters,scale=TRUE,validation="CV")
# The syntax for the pcr() function is similar to that for lm(), with a few additional options. Setting scale=TRUE has the effect of standardizing each predictor, using (6.6), prior to generating the principal components, so that the scale on which each variable is measured will not have an effect. Setting validation="CV" causes pcr() to compute the ten-fold cross-validation error for each possible value of M , the number of principal components used. The resulting fit can be examined using summary().
summary(pcr.fit)
# The CV score is provided for each possible number of components, ranging from M = 0 onwards. (We have printed the CV output only up to M = 4.) Note that pcr() reports the root mean squared error; in order to obtain the usual MSE, we must square this quantity. For instance, a root mean squared error of 352.8 corresponds to an MSE of 352.82 = 124,468.
# One can also plot the cross-validation scores using the validationplot() validation function. Using val.type="MSEP" will cause the cross-validation MSE to be plot() plotted.
validationplot(pcr.fit,val.type="MSEP")
# We see that the smallest cross-validation error occurs when M = 16 com- ponents are used. This is barely fewer than M = 19, which amounts to simply performing least squares, because when all of the components are used in PCR no dimension reduction occurs. However, from the plot we also see that the cross-validation error is roughly the same when only one component is included in the model. This suggests that a model that uses just a small number of components might suffice.
# The summary() function also provides the percentage of variance explained in the predictors and in the response using different numbers of compo- nents. This concept is discussed in greater detail in Chapter 10. Briefly, we can think of this as the amount of information about the predictors or the response that is captured using M principal components. For example, setting M = 1 only captures 38.31 % of all the variance, or information, in the predictors. In contrast, using M = 6 increases the value to 88.63 %. If we were to use all M = p = 19 components, this would increase to 100 %.
# We now perform PCR on the training data and evaluate its test set performance.
set.seed(1)
pcr.fit=pcr(Salary~., data=Hitters,subset=train,scale=TRUE, validation="CV")
validationplot(pcr.fit,val.type="MSEP")
# Now we find that the lowest cross-validation error occurs when M = 7 component are used. We compute the test MSE as follows.
pcr.pred=predict(pcr.fit,x[test,],ncomp=7)
mean((pcr.pred-y.test)^2)
# This test set MSE is competitive with the results obtained using ridge re- gression and the lasso. However, as a result of the way PCR is implemented, the final model is more difficult to interpret because it does not perform any kind of variable selection or even directly produce coefficient estimates.
# Finally, we fit PCR on the full data set, using M = 7, the number of components identified by cross-validation.
pcr.fit=pcr(y~x,scale=TRUE,ncomp=7)
summary(pcr.fit)
```
# 6.7.2 Partial Least Squares
```{r}
# We implement partial least squares (PLS) using the plsr() function, also plsr() in the pls library. The syntax is just like that of the pcr() function
set.seed(1)
pls.fit=plsr(Salary~., data=Hitters,subset=train,scale=TRUE, validation="CV")
summary(pls.fit)
validationplot(pls.fit,val.type="MSEP")
# The lowest cross-validation error occurs when only M = 2 partial least squares directions are used. We now evaluate the corresponding test set MSE.
pls.pred=predict(pls.fit,x[test,],ncomp=2)
mean((pls.pred-y.test)^2)
# The test MSE is comparable to, but slightly higher than, the test MSE obtained using ridge regression, the lasso, and PCR. Finally, we perform PLS using the full data set, using M = 2, the number of components identified by cross-validation.
pls.fit=plsr(Salary~., data=Hitters,scale=TRUE,ncomp=2)
summary(pls.fit)
```
# practice
```{r}
#9 e, f, g
# 9 In this exercise, we will predict the number of applications received using the other variables in the College data set.
# (a) Split the data set into a training set and a test set.
set.seed(1)
summary(College)
#x=model.matrix(Apps~.,College)[,-1] # Remove the first column, (Intercept)
x=model.matrix(Apps~.,College) # Does not remove the first column, (Intercept)
y=College$Apps
dim(x) # 777 17
set.seed(1)
train=sample(c(TRUE,FALSE), nrow(College),rep=TRUE)
test=(!train)
# train=sample(1:nrow(x), nrow(x)/2) # divide into two. glmnet vignet way
# test=(-train) # interesting way found in glmet vignet?
x.test=x[test,]
# x.train=x[train,]
y.test=y[test]
# y.train=y[train]
# (b) Fit a linear model using least squares on the training set, and report the test error obtained.
colnames(College)
sum(is.na(College)) # no NA
# use regsubsets()
regfit.best<-regsubsets(Apps~.,data=College[train,],nvmax=17)
summary(regfit.best)
plot(regfit.best)
#
val.errors=rep(NA,17)
for(i in 1:17){
   coefi=coef(regfit.best,id=i)
   pred=x.test[,names(coefi)]%*%coefi
   val.errors[i]=mean((College$Apps[test]-pred)^2)
}
val.errors
which.min(val.errors) # 5
# MSE. There is no predict function for this
#mean((predict(summary(rigfit.full),newx=x.test)-y.test)^2)
coef(regfit.best,5)
# from text book
predict.regsubsets=function(object,newdata,id,...){
  form=as.formula(object$call[[2]])
  mat=model.matrix(form,newdata)
  coefi=coef(object,id=id)
  xvars=names(coefi)
  mat[,xvars]%*%coefi
}
as.formula(regfit.best$call[[2]])
coef(regfit.best,id=5)

predict.regsubsets.test<-predict.regsubsets(regfit.best,newdata=College[test,],id=5)
length(predict.regsubsets.test) # 377
# back to lm
lm.train<-lm(Apps~., data=College,subset=train)
summary(lm.train)
pred.lm<-predict(lm.train,newdata=College[test,-2]) # removing "Apps" in the data, College
length(pred.lm) # 400
dim(College[test,])
# library(reshape2)
# melt.data<-melt(cbind(data.frame(pred.lm=pred.lm,pred.regsubsets=predict.regsubsets.test),Apps=College[test,2]),id="Apps")
# str(melt.data)
# library(ggplot2)
# ggplot(melt.data,aes(x=Apps,y=value,colour=variable,alpha=0.1)) + geom_point()
# MSE 
mean((pred.lm - College[test,"Apps"])^2) # 1520331 for lm
mean((predict.regsubsets.test - College[test,"Apps"])^2) # 1492114 for regsubsets
  
# (c) Fit a ridge regression model on the training set, with λ chosen by cross-validation. Report the test error obtained
library(glmnet)

#ridge.mod=glmnet(x,y,alpha=0,lambda=grid) # If alpha=0 then a ridge regression model is fit, and if alpha=1 then a lasso model is fit. 
# Recall from Chapter 3 that the least squares fitting procedure estimates β0,β1,...,βp using the values that minimize: RSS (residual sum of squares, Fig3.1, p62)
ridge.ex9=glmnet(College[train,-2],College[train,"Apps"],alpha=0) # error
ridge.ex9=glmnet(x[train,-1],College[train,"Apps"],alpha=0) # removing intercept in x
cv.ridge.ex9<-cv.glmnet(x[train,-1],College[train,"Apps"],alpha=0)
predict(cv.ridge.ex9,s=cv.ridge.ex9$lambda.min,exact=T,type="coefficients")[1:18,] # coefficients
ridge.predict.ex9<-predict(cv.ridge.ex9,s=cv.ridge.ex9$lambda.min,newx=x[test,-1])
# MSE
mean((ridge.predict.ex9 - College[test,"Apps"])^2) # 2585067

# (d) Fit a lasso model on the training set, with λ chosen by cross- validation. Report the test error obtained, along with the number of non-zero coefficient estimates.
lasso.ex9=glmnet(x[train,-1],College[train,"Apps"],alpha=1) # removing intercept in x
cv.lasso.ex9<-cv.glmnet(x[train,-1],College[train,"Apps"],alpha=1)
lasso.ex9.coef<-predict(cv.lasso.ex9,s=cv.lasso.ex9$lambda.min,exact=T,type="coefficients")[1:18,] # coefficients
lasso.ex9.coef[!lasso.ex9.coef==0] #non zero
lasso.ex9.coef[lasso.ex9.coef!=0] #non zero. this is also OK.
lasso.predict.ex9<-predict(cv.lasso.ex9,s=cv.ridge.ex9$lambda.min,newx=x[test,-1])
# MSE
mean((lasso.predict.ex9 - College[test,"Apps"])^2) # 2010635. smaller than ridge
## starting from this
# (e) Fit a PCR model on the training set, with M chosen by cross- validation. Report the test error obtained, along with the value of M selected by cross-validation.
library(pls)
set.seed(2)
pcr.fit=pcr(Apps~., data=College,subset=train,scale=TRUE,validation="CV")
validationplot(pcr.fit,val.type="MSEP") # which M? 5?
# Compute the test MSE.
summary(pcr.fit)
pcr.pred=predict(pcr.fit,newdata=x[test,][,-1],ncomp=5)
mean((pcr.pred-y.test)^2) # 3519875, which is larger than lasso
# (f) Fit a PLS model on the training set, with M chosen by cross- validation. Report the test error obtained, along with the value of M selected by cross-validation.
set.seed(1)
pls.fit=plsr(Apps~., data=College,subset=train,scale=TRUE, validation="CV")
summary(pls.fit)
validationplot(pls.fit,val.type="MSEP")
# The lowest cross-validation error occurs when only M = 2 partial least squares directions are used. We now evaluate the corresponding test set MSE.
pls.pred=predict(pls.fit,x[test,][,-1],ncomp=3)
mean((pls.pred-y.test)^2) # 2982036

# (g) Comment on the results obtained. How accurately can we pre- dict the number of college applications received? Is there much difference among the test errors resulting from these five approaches?
## lm is the best?!
mean((pred.lm - College[test,"Apps"])^2) # 1520331 for lm
mean((ridge.predict.ex9 - College[test,"Apps"])^2) # 2585067
mean((lasso.predict.ex9 - College[test,"Apps"])^2) # 2010635. smaller than ridge
mean((pcr.pred-y.test)^2) # 3519875, which is larger than lasso
mean((pls.pred-y.test)^2) # 2982036

```
#11 a,b,c, for all, we already did this, update for PCR
```{r}
# We will now try to predict per capita crime rate in the Boston data set.
# (a) Try out some of the regression methods explored in this chapter, such as best subset selection, the lasso, ridge regression, and PCR. Present and discuss results for the approaches that you consider.
library(MASS)
summary(Boston)
?Boston
sum(is.na(Boston)) # zero
x=model.matrix(crim~.,Boston) # Does not remove the first column, (Intercept)
y=Boston$crim
dim(x) # 
set.seed(1)
train=sample(c(TRUE,FALSE), nrow(Boston),rep=TRUE)
test=(!train)
# train=sample(1:nrow(x), nrow(x)/2) # divide into two. glmnet vignet way
x.test=x[test,]
y.test=y[test]
# lm
lm.train<-lm(crim~., data=Boston,subset=train)
summary(lm.train)
pred.lm<-predict(lm.train,newdata=Boston[test,-1]) 
length(pred.lm) # 254
dim(Boston[test,]) # 254 14
# MSE lm
mean((pred.lm - Boston[test,"crim"])^2) # 57.13161
############ under construction ###########
# regsub
regfit.best<-regsubsets(crim~.,data=Boston[train,],nvmax=14)
summary(regfit.best)
plot(regfit.best)
#
val.errors=rep(NA,17)
for(i in 1:17){
   coefi=coef(regfit.best,id=i)
   pred=x.test[,names(coefi)]%*%coefi
   val.errors[i]=mean((College$Apps[test]-pred)^2)
}
val.errors
which.min(val.errors) # 5
# MSE. There is no predict function for this
#mean((predict(summary(rigfit.full),newx=x.test)-y.test)^2)
coef(regfit.best,5)
# from text book
predict.regsubsets=function(object,newdata,id,...){
  form=as.formula(object$call[[2]])
  mat=model.matrix(form,newdata)
  coefi=coef(object,id=id)
  xvars=names(coefi)
  mat[,xvars]%*%coefi
}
as.formula(regfit.best$call[[2]])
coef(regfit.best,id=5)
predict.regsubsets.test<-predict.regsubsets(regfit.best,newdata=College[test,],id=5)

# (b) Propose a model (or set of models) that seem to perform well on this data set, and justify your answer. Make sure that you are evaluating model performance using validation set error, cross- validation, or some other reasonable alternative, as opposed to using training error.

# (c) Does your chosen model involve all of the features in the data set? Why or why not?

```
