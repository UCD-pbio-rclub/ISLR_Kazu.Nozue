---
title: "chapter 7: March_21_2018"
author: "Kazu"
date: "3/21/2018"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,error=TRUE)
```
# Chapter 7 Lab: Non-linear Modeling

```{r}
# 7.8 Lab: Non-linear Modeling
library(ISLR)
attach(Wage)
#7.8.1 Polynomial Regression and Step Functions
fit=lm(wage~poly(age,4),data=Wage)
coef(summary(fit))
fit2=lm(wage~poly(age,4,raw=T),data=Wage)
coef(summary(fit2))
fit2a=lm(wage~age+I(age^2)+I(age^3)+I(age^4),data=Wage)
coef(fit2a)
# This does the same more compactly, using the cbind() function for building a matrix from a collection of vectors; any function call such as cbind() inside a formula also serves as a wrapper.
fit2b=lm(wage~cbind(age,age^2,age^3,age^4),data=Wage)
# We now create a grid of values for age at which we want predictions, and then call the generic predict() function, specifying that we want standard errors as well.
agelims=range(age) # 
age.grid=seq(from=agelims[1],to=agelims[2])
preds=predict(fit,newdata=list(age=age.grid),se=TRUE)
se.bands=cbind(preds$fit+2*preds$se.fit,preds$fit-2*preds$se.fit)
#par(mfrow=c(1,2),mar=c(4.5,4.5,1,1),oma=c(0,0,4,0))
plot(age,wage,xlim=agelims,cex=.5,col="darkgrey")
title("Degree-4 Polynomial",outer=T)
lines(age.grid,preds$fit,lwd=2,col="blue")
matlines(age.grid,se.bands,lwd=1,col="blue",lty=3)
preds2=predict(fit2,newdata=list(age=age.grid),se=TRUE)
max(abs(preds$fit-preds2$fit))
fit.1=lm(wage~age,data=Wage)
fit.2=lm(wage~poly(age,2),data=Wage)
fit.3=lm(wage~poly(age,3),data=Wage)
fit.4=lm(wage~poly(age,4),data=Wage)
fit.5=lm(wage~poly(age,5),data=Wage)
anova(fit.1,fit.2,fit.3,fit.4,fit.5)
coef(summary(fit.5))
(-11.983)^2
fit.1=lm(wage~education+age,data=Wage)
fit.2=lm(wage~education+poly(age,2),data=Wage)
fit.3=lm(wage~education+poly(age,3),data=Wage)
anova(fit.1,fit.2,fit.3)
# Next we consider the task of predicting whether an individual earns more than $250,000 per year. We proceed much as before, except that first we create the appropriate response vector, and then apply the glm() function using family="binomial" in order to fit a polynomial logistic regression model.
fit=glm(I(wage>250)~poly(age,4),data=Wage,family=binomial) 
# Note that we again use the wrapper I() to create this binary response variable on the fly. The expression wage>250 evaluates to a logical variable containing TRUEs and FALSEs, which glm() coerces to binary by setting the TRUEs to 1 and the FALSEs to 0.
preds=predict(fit,newdata=list(age=age.grid),se=T)
pfit=exp(preds$fit)/(1+exp(preds$fit))
se.bands.logit = cbind(preds$fit+2*preds$se.fit, preds$fit-2*preds$se.fit)
se.bands = exp(se.bands.logit)/(1+exp(se.bands.logit))
# Note that we could have directly computed the probabilities by selecting the type="response" option in the predict() function.
preds=predict(fit,newdata=list(age=age.grid),type="response",se=T)
# However, the corresponding confidence intervals would not have been sensible because we would end up with negative probabilities! Finally, the right-hand plot from Figure 7.1 was made as follows:
plot(age,I(wage>250),xlim=agelims,type="n",ylim=c(0,.2))
points(jitter(age), I((wage>250)/5),cex=.5,pch="|",col="darkgrey")
lines(age.grid,pfit,lwd=2, col="blue")
matlines(age.grid,se.bands,lwd=1,col="blue",lty=3)
# We have drawn the age values corresponding to the observations with wage values above 250 as gray marks on the top of the plot, and those with wage values below 250 are shown as gray marks on the bottom of the plot. We used the jitter() function to jitter the age values a bit so that observations with the same age value do not cover each other up. This is often called a rug plot.In order to fit a step function, as discussed in Section 7.2, we use the cut() function.
table(cut(age,4))
fit=lm(wage~cut(age,4),data=Wage)
coef(summary(fit))
# Here cut() automatically picked the cutpoints at 33.5, 49, and 64.5 years of age. We could also have specified our own cutpoints directly using the breaks option. The function cut() returns an ordered categorical variable; the lm() function then creates a set of dummy variables for use in the re- gression. The age<33.5 category is left out, so the intercept coefficient of $94,160 can be interpreted as the average salary for those under 33.5 years of age, and the other coefficients can be interpreted as the average addi- tional salary for those in the other age groups. We can produce predictions and plots just as we did in the case of the polynomial fit.
```
#7.8.2 Splines
```{r}
library(splines)
fit=lm(wage~bs(age,knots=c(25,40,60)),data=Wage)
pred=predict(fit,newdata=list(age=age.grid),se=T)
plot(age,wage,col="gray")
lines(age.grid,pred$fit,lwd=2)
lines(age.grid,pred$fit+2*pred$se,lty="dashed")
lines(age.grid,pred$fit-2*pred$se,lty="dashed")
dim(bs(age,knots=c(25,40,60)))
dim(bs(age,df=6))
attr(bs(age,df=6),"knots")
<<<<<<< HEAD
# In this case R chooses knots at ages 33.8, 42.0, and 51.0, which correspond to the 25th, 50th, and 75th percentiles of age. The function bs() also has a degree argument, so we can fit splines of any degree, rather than the default degree of 3 (which yields a cubic spline). 
# In order to instead fit a natural spline, we use the ns() function. Here ns() we fit a natural spline with four degrees of freedom.
fit2=lm(wage~ns(age,df=4),data=Wage)
pred2=predict(fit2,newdata=list(age=age.grid),se=T)
lines(age.grid, pred2$fit,col="red",lwd=2)
# In order to fit a smoothing spline, we use the smooth.spline() function. smooth. Figure 7.8 was produced with the following code:
=======
fit2=lm(wage~ns(age,df=4),data=Wage)
pred2=predict(fit2,newdata=list(age=age.grid),se=T)
lines(age.grid, pred2$fit,col="red",lwd=2)
>>>>>>> 3a27b75e471d386043be3c0b8b6a8cb0d5278640
plot(age,wage,xlim=agelims,cex=.5,col="darkgrey")
title("Smoothing Spline")
fit=smooth.spline(age,wage,df=16)
fit2=smooth.spline(age,wage,cv=TRUE)
fit2$df
lines(fit,col="red",lwd=2)
lines(fit2,col="blue",lwd=2)
legend("topright",legend=c("16 DF","6.8 DF"),col=c("red","blue"),lty=1,lwd=2,cex=.8)
plot(age,wage,xlim=agelims,cex=.5,col="darkgrey")
title("Local Regression")
fit=loess(wage~age,span=.2,data=Wage)
fit2=loess(wage~age,span=.5,data=Wage)
lines(age.grid,predict(fit,data.frame(age=age.grid)),col="red",lwd=2)
lines(age.grid,predict(fit2,data.frame(age=age.grid)),col="blue",lwd=2)
legend("topright",legend=c("Span=0.2","Span=0.5"),col=c("red","blue"),lty=1,lwd=2,cex=.8)
detach()
```
# Practice
#6. In this exercise, you will further analyze the Wage data set considered throughout this chapter.
```{r}
## (a) Perform polynomial regression to predict wage using age. Use cross-validation to select the optimal degree d for the polynomial. What degree was chosen, and how does this compare to the results of hypothesis testing using ANOVA? Make a plot of the resulting polynomial fit to the data.
attach(Wage)
summary(Wage)
summary(is.na(Wage)) # no NAs.
set.seed(1)
train=sample(c(TRUE,FALSE), nrow(Wage),rep=TRUE)
test=(-train)
# x=model.matrix(wage~.,Wage) # Does not remove the first column, (Intercept)
y=Wage$wage
y.test=y[test]
# 
fit.d1=lm(wage~poly(age,1),data=Wage[train,])
fit.d2=lm(wage~poly(age,2),data=Wage[train,])
fit.d3=lm(wage~poly(age,3),data=Wage[train,])
fit.d4=lm(wage~poly(age,4),data=Wage[train,])
fit.d5=lm(wage~poly(age,5),data=Wage[train,])
fit.d6=lm(wage~poly(age,6),data=Wage[train,])
fit.d7=lm(wage~poly(age,7),data=Wage[train,])

# prediction
preds.d1=predict(fit.d1,newdata=Wage[test,])
preds.d2=predict(fit.d2,newdata=Wage[test,])
preds.d3=predict(fit.d3,newdata=Wage[test,])
preds.d4=predict(fit.d4,newdata=Wage[test,])
preds.d5=predict(fit.d5,newdata=Wage[test,])
preds.d6=predict(fit.d6,newdata=Wage[test,])
preds.d7=predict(fit.d7,newdata=Wage[test,])

# MSE
mean((preds.d1-Wage[test,"wage"])^2)
mean((preds.d2-Wage[test,"wage"])^2)
mean((preds.d3-Wage[test,"wage"])^2)
mean((preds.d4-Wage[test,"wage"])^2)
mean((preds.d5-Wage[test,"wage"])^2)
mean((preds.d6-Wage[test,"wage"])^2)
mean((preds.d7-Wage[test,"wage"])^2)
# ANOVA
anova(fit.d1,fit.d2,fit.d3,fit.d4,fit.d5,fit.d6,fit.d7)

# how about to use boot()? cv.glm()
library(boot)
set.seed(2)
# K=5, i is degree testing 1 to 10
all.deltas = rep(NA, 10)
for (i in 1:10) {
  glm.fit = glm(wage~poly(age, i), data=Wage)
  all.deltas[i] = cv.glm(Wage, glm.fit, K=30)$delta[2]
}
# explanation of cv.glm()$delta from help
# delta: A vector of length two. The first component is the raw cross-validation estimate of prediction error. The second component is the adjusted cross-validation estimate. The adjustment is designed to compensate for the bias introduced by not using leave-one-out cross-validation.
all.deltas
library(ggplot2)
qplot(1:10,all.deltas,geom="line",xlab="degree")

<<<<<<< HEAD
## see Julin's tibble way

=======
>>>>>>> 3a27b75e471d386043be3c0b8b6a8cb0d5278640
## (b) Fit a step function to predict wage using age, and perform cross- validation to choose the optimal number of cuts. Make a plot of the fit obtained.
fit.lm.cut2=lm(wage~cut(age,2),data=Wage[train,])
fit.lm.cut3=lm(wage~cut(age,3),data=Wage[train,])
fit.lm.cut4=lm(wage~cut(age,4),data=Wage[train,])
fit.lm.cut5=lm(wage~cut(age,5),data=Wage[train,])
# prediction
preds.lm.cut2=predict(fit.lm.cut2,newdata=Wage[test,])
preds.lm.cut3=predict(fit.lm.cut3,newdata=Wage[test,])
preds.lm.cut4=predict(fit.lm.cut4,newdata=Wage[test,])
preds.lm.cut5=predict(fit.lm.cut5,newdata=Wage[test,])
# MSE
mean((preds.lm.cut2-Wage[test,"wage"])^2)
mean((preds.lm.cut3-Wage[test,"wage"])^2)
mean((preds.lm.cut4-Wage[test,"wage"])^2)
mean((preds.lm.cut5-Wage[test,"wage"])^2)
<<<<<<< HEAD
# anova

=======
>>>>>>> 3a27b75e471d386043be3c0b8b6a8cb0d5278640
# using boot(), cut 2 to 10
all.deltas = rep(NA, 9)
for (i in 2:10) {
  glm.fit = glm(wage~cut(age,i),data=Wage)
  all.deltas[i] = cv.glm(Wage, glm.fit, K=30)$delta[2]
} # does not work
for (i in 2:10) {
  Wage$cut2<-cut(age,i)
  glm.fit = glm(wage~cut2,data=Wage)
  all.deltas[i] = cv.glm(Wage, glm.fit, K=30)$delta[2]
} 
all.deltas
library(ggplot2)
qplot(1:10,all.deltas,geom="line",xlab="degree")
# plot the fit
Wage$cut3<-cut(Wage$age,8)
glm.fit <- glm(wage~cut3,data=Wage)
Wage$predict.wage<-predict(glm.fit,cut3)
ggplot(Wage,aes(x=age,y=wage)) + geom_point(alpha=0.1) + geom_step(aes(x=age,y=predict.wage,color=I("red")))
detach()
```

```{r}
# 7. The Wage data set contains a number of other features not explored in this chapter, such as marital status (maritl), job class (jobclass), and others. Explore the relationships between some of these other predictors and wage, and use non-linear fitting techniques in order to fit flexible models to the data. Create plots of the results obtained, and write a summary of your findings.
attach(Wage)
?pairs
pairs(Wage)
names(Wage)
<<<<<<< HEAD
library(ggplot2)
=======
>>>>>>> 3a27b75e471d386043be3c0b8b6a8cb0d5278640
ggplot(Wage,aes(as.factor(year),wage))+geom_violin() # no effects
ggplot(Wage,aes(maritl,wage))+geom_violin() # has effects
ggplot(Wage,aes(race,wage))+geom_violin() # has effects
ggplot(Wage,aes(education,wage))+geom_violin() # has strong effects
ggplot(Wage,aes(region,wage))+geom_violin() # only Middle Atlantic
ggplot(Wage,aes(jobclass,wage))+geom_violin() # has effects
ggplot(Wage,aes(health,wage))+geom_violin() # has effects
ggplot(Wage,aes(health_ins,wage))+geom_violin() # has strong effects
ggplot(Wage,aes(logwage,wage))+geom_point() # just transformation

# non-linear regression using poly() and estimate coefficient by ridge or lasso
# which degree I need to use?

<<<<<<< HEAD


=======
>>>>>>> 3a27b75e471d386043be3c0b8b6a8cb0d5278640
detach()
```
# 8. Fit some of the non-linear models investigated in this chapter to the Auto data set. Is there evidence for non-linear relationships in this data set? Create some informative plots to justify your answer.

```{r}
Auto
pairs(Auto)
<<<<<<< HEAD
library(ggplot2) 
library(GGally) # from Min Yao repo
ggpairs(Auto[-9])

=======
>>>>>>> 3a27b75e471d386043be3c0b8b6a8cb0d5278640
```
# 9. This question uses the variables dis (the weighted mean of distances to five Boston employment centers) and nox (nitrogen oxides concen- tration in parts per 10 million) from the Boston data. We will treat dis as the predictor and nox as the response.

```{r}
## (a) Use the poly() function to fit a cubic polynomial regression to predict nox using dis. Report the regression output, and plot the resulting data and polynomial fits.
library(MASS);head(Boston)
<<<<<<< HEAD
i<-3
fit.glm<-glm(nox~poly(dis,i),data=Boston)
fit.glm
predict.nox<-predict(fit.glm,newdata=Boston$dis) # does not work...
seq(range(Boston$dis)[1],range(Boston$dis)[2])
predict.nox1<-predict(fit.glm,newdata=list(dis=seq(range(Boston$dis)[1],range(Boston$dis)[2]))) # does work
Boston$predict.nox<-predict(fit.glm,newdata=list(dis=Boston$dis),se.fit=TRUE)$fit # does work...
Boston$predict.nox.se<-predict(fit.glm,newdata=list(dis=Boston$dis),se.fit=TRUE)$se.fit # does work...
#
predict.nox<-predict(fit.glm) # does work...
# 
ggplot(Boston,aes(dis,nox))+geom_point(alpha=0.1) + geom_line(aes(dis,predict.nox))
# or using grid for predicted value
ggplot(Boston,aes(dis,nox))+geom_point(alpha=0.1) + geom_line(data=data.frame(x=1:11,y=predict.nox1),aes(x,y))
# or ignore new data
ggplot(Boston,aes(dis,nox))+geom_point(alpha=0.1) + geom_point(aes(dis,predict(fit.glm)),color=I("red"),alpha=0.1)
# 
ggplot(Boston,aes(dis,nox))+geom_point(alpha=0.1) + geom_ribbon(aes(ymin=predict.nox-predict.nox.se,ymax=predict.nox+predict.nox.se))

##  (b) Plot the polynomial fits for a range of different polynomial degrees (say, from 1 to 10), and report the associated residual sum of squares.
for(i in 2:10) {
fit.glm<-glm(nox~poly(dis,i),data=Boston)
Boston[,14+i-1]<-predict(fit.glm) 
names(Boston)[14+i-1]<-paste("degree",i)
  }

# reformat Boston
Boston %>% dplyr::select(dis,nox,contains("degree")) %>% gather(dis,"value",2:10) -> Boston.gather
head(Boston.gather) # does not work well

ggplot(aes(dis,-dis))+geom_point(alpha=0.1) + geom_ribbon(aes(ymin=predict.nox-predict.nox.se,ymax=predict.nox+predict.nox.se))
=======

##  (b) Plot the polynomial fits for a range of different polynomial degrees (say, from 1 to 10), and report the associated residual sum of squares.
>>>>>>> 3a27b75e471d386043be3c0b8b6a8cb0d5278640


## (c) Perform cross-validation or another approach to select the opti- mal degree for the polynomial, and explain your results.

## (d) Use the bs() function to fit a regression spline to predict nox using dis. Report the output for the fit using four degrees of freedom. How did you choose the knots? Plot the resulting fit.

## (e) Now fit a regression spline for a range of degrees of freedom, and plot the resulting fits and report the resulting RSS. Describe the results obtained.

## (f) Perform cross-validation or another approach in order to select the best degrees of freedom for a regression spline on this data. Describe your results.
```